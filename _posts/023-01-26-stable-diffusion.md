---
title: 'Personalised style transfer with Stable Diffusion'
date: 2023-01-26 00:00:00
description: Generating images with AI
featured_image: '/images/demo/demo-landscape.jpg'
---

The internet is awash with images generated by AI models like Midjourney, DALLE, and Stable Diffusion. But the images that accompany articles on it tend to be predictably same-y. Sci-fi themes, Blade Runner dystopian cities, anime. 'Photorealistic' images of women that have the facetuning built in.

I wanted to experiment with generating some different looking images using a form of style transfer. I did this using textual inversion on Stable Diffusion.

For this experiment, I wanted to push the model into recreating the textures, palettes and ornate style of children's illustrator Brian Wildsmith. Wildsmith made beautiful books and posters using a range of media, from scratchy pen and ink to rich gauche. One of the most striking things about his work is the use of colour -- fuscias, emerald greens, and frequent use of geometic patterns. I wanted to see how well the model would learn and apply these themes. This isn't the simplest way to showcase sty;e transfer, since Wildsmith's style is very broad, spanning a lifetime of work.

Here are some examples of original Wildsmith:


<!-- images--> 

<style>
.gallery {
  width: 800px;
}

figcaption {
  display: block;
  margin-top: 1em;
  font-style: italic;
  text-align: center;
}


</style>

<figure>
    <div class="gallery" data-columns="1">
        <img src="/images/stable_diffusion/Robert-Louis-Stevenson-a-childs-garden-of-verses-Escape-at-Bedtime-Brian-Wildsmith.jpg">
        <img src="/images/stable_diffusion/Jungle-party-original-illustration-P1&2-brian-wildsmith.jpg">
        <img src="/images/stable_diffusion/burning-sun.jpg">
        <img src="/images/stable_diffusion/sun-closeup.jpg">
        <img src="/images/stable_diffusion/sleeping-beauty-poster-by-brian-wildsmith-for-franklin-watts-NYC.jpg">
    </div>

<figcaption>Original images by Brian Wildsmith</figcaption>
</figure>

# Is any of this OK?
Before I start throwing around derivative images from a model trained on Brian Wildsmith's work, I want to make it clear that the AI-generated images don't hold a candle to the real thing, in terms of ingenuity, creativity, or the simple human communication that is essential to art.

There are many ethical and normative questions that have been raised concerning so-called AI Art. A significant worry is that AI is plagiarising artists. Another is that it is coming for their jobs. I don't see any evidence that AI is replacing artists. But, of course, art represents only a tiny fraction of the professions that create graphics, including commercial photography and graphic design. We should be realistic about the fact that some of their work could be automated with generative models. However, art -- construed broadly, and including among a huge number of other crafts, illustration, photography, and digital art -- is very different from those other activities, and so it is puzzling why generative models are presented as uprooting *art*. Obviously, also, art has no clear demarcation, but there is nevertheless a spectrum, from fine art, through decorative art and design, to functional images like icons or stock photography. 

To be honest, I think practising artists are probably not that concerned about being replaced by AI. There was a minor scandal at the end of last year when a digital art prize was won by an entry generated by Midjourney. The winner [gloated](https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html) "Art is dead, dude. Itâ€™s over. A.I. won. Humans lost."  I don't see how you can hold this view without knowing both very little about the technology and very little about art.

Enough about art, back to the tech.

## Textual inversion on a style
There is a lot of hype around the idea of 'fine-tuning' large foundation models to make them applicible to custom domains. This doesn't work so well in practice, due amongst other things to the alarmingly named 'catastrophic forgetting'. But there are reliable methods that can be used to edit your local copy of a pretrained model, such as textual inversion.

Textual inversion enables you to add representational capacity to a pretrained text-to-image model by adding a new pseudo-word to the embedding space of the model, and teaching the model what visual pattern the word represents. This could be an image of a specific object or a style, as in my use case. The method effectively teaches the model a new word by examples.

The paper on textual inversion (Gal et al, 2022) suggests 3-5 images of an object is sufficient to embed it into the model. I started off with about 6 images in my training set, and then increased this considerably, first to 136 and ultimately to 318.

## Data and setup
I used Automatic1111's webui to train the textual inversion on Stable Diffusion v1.4.

My dataset was around 300 images I scanned from Brian Wildsmith books and found online. I named each file with a brief description of the content. The filenames are used in training to generate prompts of the form `"a painting of [filewords] by [name]"`, where `[filewords]` come from the filename and `[name]` is the new pseudo-word.


Images were cropped to 512x512. One hurdle was how to programmatically crop the images in a way that preserved features of interest. Centre-copping meant that the resulting models generated images missing the tops of heads. For speed, I used a feature of the webui to programatically crop [based on detected focal point](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/3139) in combination with splitting large images into smaller, overlapping tiles, but this was not entirely satisfactory.


To generate the embedding, I used 16 vectors per token rather than the default of 1, because there are lots of different aspects of the training set I wanted to capture and represent with my new token (use of geometric patterns, textures, colour combinations etc). Increasing the value yielded improvements in texture, for instance, enabling the representation of brushstrokes, as shown in examples below.


The textual inversion implementation in Gal et al uses an initialisation text, which is set to a "single word coarse descriptor" of the new object being introduced. For instance, to add a particular dog to the model, initialise with "dog". In the case of style transfer, it wasn't clear what word to use, since the images in the training set are only related by style and not by subject matter. I went with `illustration`. 

## Results
Here's a couple of images that were generated randomly during training of the new embedding. 

<style>
.gallery {
  width: 1000px;
}
</style>
<div class="gallery" data-columns="2">
    <img src="/images/stable_diffusion/generated/owl_7000steps.png">
    <img src="/images/stable_diffusion/generated/elephant_9000steps.png">
</div>

The geometric pattern in the background of the elephant is frequently generated by my model. It appears in the training set in some of Wildsmith's illustrations of abstract shapes. For instance, from his counting book:


<style>
.gallery {
  width: 1000px;
}
</style>

<figure>
<div class="gallery" data-columns="3">
    <img src="/images/stable_diffusion/number-1-from-brian-wildsmiths-123.png">
    <img src="/images/stable_diffusion/number-2-from-brian-wildsmiths-123.png">
    <img src="/images/stable_diffusion/number-three-from-brian-wildsmiths-123.png">
</div>

<figcaption>Original images from 123 by Brian Wildsmith (1970)</figcaption>
</figure>

The decorative design on the elephant's trunk does not appear in the training set, but was created by the model. The closest thing in the training set, in terms of content, is the following elephant with a decorative saddle. The generated image is clearly different and draws from different aspects of the Wildsmith style:


<figure>
<p align="center"> 
<img src="/images/stable_diffusion/elephant-with-decorative-clothes.png" width="300">
</p>
<figcaption>From What the Moon Saw by Brian Wildsmith</figcaption>
</figure>


The following sets of examples were created using the same prompts, with embeddings using 1 and 16 vectors per token respectively, though different random seeds in each case. The version with 16 vectors per token captures more of the texture of the pencil and paintbrush strokes in the original images, as well as some unintended artefacts of the training data like page creases, photographic glare and shadows.

Results were generated with 20-80 sampling steps. I raised CFG scale to 14 from the default 7. Theoretically this should force closer adherence to the prompt; I found it gave better looking results, although it also tended to reproduce more of the unwanted artefacts too.

<figure>
<div class="gallery" data-columns="3">
    <img src="/images/stable_diffusion/generated/00053-2402323073-colourful-oil-painting-of-a-cat-in-the-style-of-brian_wildsmith.png">
    <img src="/images/stable_diffusion/generated/00062-3942776645-a-magnificent-multicoloured-sun.png">
    <img src="/images/stable_diffusion/generated/1_vector_00151-64173078-a decorative-cottage-in-forest-surrounded-by- green-trees-and-flowers-the-style-of-zzl_brian_wildsmith_318_noresize_preproce.png">
</div>
</figure>
Using 16 vectors per token (sadly the cat image was a victim of the cropping issue):

<figure>
<div class="gallery" data-columns="3">
    <img src="/images/stable_diffusion/generated/16vectors_00135-1247160374-a colourful oil painting-of-a-cat-in-the-style-of-brian_wildsmith.png">
    <img src="/images/stable_diffusion/generated/16vectors_00137-1068457987-a-magnificent-multicoloured-sun-in-the-style-of-brian_wildsmith.png">
    <img src="/images/stable_diffusion/generated/16vectors_00153-1093123542-a decorative-cottage-in-forest-surrounded-by-green-trees-and-flowers-in-style-of-jwll_wildsmith.png">
</div>
<figcaption>Images generated by Stable Diffusion with textual inversion)</figcaption>
</figure>

To show the effect on varying `cfg_scale`, see the sequence of images below, for `cfg_scale` ranging from 0 and 30 with a fixed seed. The model was prompted to draw a city of tents in the desert under a full moon, in Wildsmith's style. I think this reveals that `cfg_scale` isn't having all that much effect on the quality of the generated image, once it gets past about 1, except for a few mid-range values where it fails to render the moon. Otherwise, it's just gently exploring a set of similar points in the space.


<div class="gallery" data-columns="1">
    <img src="https://storage.googleapis.com/hodesdon-com/tent_city.gif">
</div>

This time, a house with a mountain range in the background. The image flickers between different placements of the house, leaving it out altogether for large `cfg_scale` values.

<div class="gallery" data-columns="1">
    <img src="https://storage.googleapis.com/hodesdon-com/mountain_range.gif">
</div>

## Classifier-free guidance for style transfer
One parameter I experimented with was the value controlling classifier free guidance. In Automatic1111's web UI, this parameter is `cfg_scale`. Classifier free guidance is a technique for varying the closeness with which the generated results adhere to the prompt. I wanted to understand what this means in the context of style transfer.


I'll briefly recap what classifier free guidance does. Let's begin with classifier guidance, introduced by Dhariwal and Nichol (2021) as the method that gave diffusion models the edge over GANs in image synthesis. I found [Beanne's blog post](https://benanne.github.io/2022/05/26/guidance.html) on guidance gave a helpful motivation for both methods.

To see what classifier guidance does, let's recap what a conditional diffusion model like stable diffusion does.

It's easiest to start from inference. At inference time, when you ask your model to produce an image in response to your prompt, the model is running a reverse diffusion process to form an image from noise. The problem is set up as removing the noise in increments: $x_{T}$ is pure noise, $x_{0}$ is noiseless, and for each $t \in [0, T]$, $x_{t-1}$ is slightly less noisy version than $x_{t}$. We are able to do this denoising because the model is trained to denoise images.

It is intractable to compute the probability of $x_{t-1}$ given $x_{t}$ because it depends on the entire dataset. However, we can train a model $\epsilon_{\theta}(x_{t})$ that approximates these conditional probabilities. 


In training, the model is shown random pairs of noised images, $x_{t}$ and timesteps $t$, and it learns predict the noise, $\epsilon_{t}$ which is equivalent to predicting the denoised image $x_{t-1}$ from $x_{t}$ and timestep $t$. This is the denoising model. While $\epsilon_{t}$ is the model, $p_{\theta}(x_{t})$  is the probability distribution of the model's outputs. There is just one additional detail: we're considering _conditional_ diffusion models, which is to say, denoising is conditioned on some text prompt $y$. So we are actually dealing with the conditional probability $p_{\theta}(x_{t}\vert  y,t)$. 

Under certain assumptions, the distribution $p_{\theta}$ is a Gaussian. You can read more of the details in [Lilian Weng's blog post on diffusion models](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#reverse-diffusion-process). In the remainder, we'll follow the notational convention of suppressing the timestep annotations, and assume for each timestep $t$ a function $p_{\theta}(x\vert y) = p_{\theta}(x_{t}\vert y, t)$


The insight of classifier guidance is that we can modify the sampling process from $p_{\theta}$ in such a way that the resulting image is more closely aligned to the prompt.

The way that this works in practice involves mixing in an additional factor, parameterised by a scalar coefficient. The extra factor is contributed by an additional model, specifially a classifer. It's not a model that is especially hard to train, but it does add an extra overhead in pursuit of higher quality images.

Let's see why a classifier gives us something helpful in sampling from the diffusion model.

In simple terms, we want to know how much the log-probability of an image $x$ given $y$ changes when we change $x$. This is the gradient of the log-probability, $ \nabla_{x} log p_{\theta}(x\vert y)$. Then we can iteratively move towards such an x.

The score function, which is what is optimised at inference time, [[[IS IT???]]] approximates $ \nabla_{x} log p_{\theta}(x\vert y)$.


We can apply Bayes rule to rewrite the score function as equation (*) below: 

$p_{\theta}(x\vert y) = \frac{p_{\theta}(y\vert x).p_{\theta}(x)}{p_{\theta}(y)}$


Then, taking gradients w.r.t. $x$, and distributing the log:

$\Rightarrow\nabla_{x}log(p_{\theta}(x\vert y)) = \nabla_{x}log\frac{p_{\theta}(y\vert x)p_{\theta}(x)}{p_{\theta}(y)}$

$\Rightarrow \nabla_{x} log p_{\theta}(x\vert y) = \nabla_{x}log (p_{\theta}(y\vert x)) + \nabla_{x} log(p_{\theta}(x)) - \nabla_{x} log(p_{\theta}(y))$

$\Rightarrow \nabla_{x} log p_{\theta}(x\vert y) = \nabla_{x}log (p_{\theta}(y\vert x)) + \nabla_{x} log(p_{\theta}(x))   \space \space\space \space\space \space (*)$ 

Since $p_{\theta}(y)$ is not a function in x, its gradient w.r.t. $x$ is 0.

Notice that via the rearrangement above, we are expressing the gradient of the conditional probability $log(p_{\theta}(x\vert y))$ in terms of the gradients of the _unconditional probability_ $log(p_{\theta}(x))$ and the _classifier_ $log(p_{\theta}(y\vert x))$.
It is a classifier because it is just the thing that a vanilla classification model learns: the probability of the label given the data. In this way, the diffusion model's score function can be decomposed into an unconditional probability and a classifier.


Classifier guidance tweaks the score function, to introduce a coefficient $s$ controlling the classifier term $\nabla_{x}log (p_{\theta}(y\vert x))$. This modification can be equivalently interpreted as perturbing the mean of the Gaussian distribution $p_{\theta}$ models; for details see Nichol et al (2022).

So, what is the effect on our model outputs of varying $s$ in the new score function? Well, we know that $s\nabla_{x}log (p_{\theta}(y\vert x)) = \nabla_{x} log \frac{1}{Z}p_{\theta}(y\vert x)^{s}$ for a constant $Z$. This is to say, multiplying the log probability of $y$ given $x$ by $s$ is proportional to raising the renormalised log probability to the power $s$. And this exponentiation has the effect of disproportionately increasing its larger values, amplifying the modes of the distribution. So, for $s>1$, the $x$ that maximises $s\nabla_{x}log (p_{\theta}(y\vert x)) + \nabla_{x} log(p_{\theta}(x))$ will be closer to the mode of the distribution $p_{\theta}(y\vert x)$, so closer to the class label $y$.

This is why classifier guidance is known to have the effect of boosting the fidelity of diffusion model outputs at the expense of diversity. And it is called classifier guidance because you are guiding the score function closer to the modes of a classifier based on the prompt.

Note that in order to apply the modified score function in practice, you need to obtain a classifier $p_{\theta}(y\vert x)$, which is a model trained on noisy inputs to predict the Imagenet classes. [TODO -- is classifier really referred to as p-sub-theta or does it deserve own distro annotation?]

Classifier free guidance (Ho & Salimans (2021)) is a development of classifier guidance. It pushes the model in the same direction as classifier guidance, but avoids the need to train a specialised classifier.

To use  classifier-free guidance, during training we replace the label $y$ in a condional diffusion model with a null label, $\emptyset$, a fixed proportion of the time, typically 10-20%. 


Recall that the de-noising process is modeled by $\epsilon_{\theta}(x_{t}\vert y)$. We replace this with $\hat{\epsilon}_{\theta}(x_{t}\vert y)$, a weighted combination of the original conditional denoising model and an unconditional denoising model as follows:


$$\hat{\epsilon}_{\theta}(x_{t}\vert y) = \epsilon_{\theta}(x_{t}\vert \emptyset) + s(\epsilon_{\theta}(x_{t}\vert y) - \epsilon_{\theta}(x_{t}\vert \emptyset))$$


For $s=0$, $\hat{\epsilon}_{\theta}$ is just the unconditional denoising model, and for $s=1$, it is the original conditional denoising model. But as $s>1$ increases, the model is a mixture of the conditional and unconditional, increasingly weighted towards the conditional model.


I used stable-diffusion v1.4, which was trained with [10% text-conditioning dropout](https://huggingface.co/CompVis/stable-diffusion-v1-4), allowing me to use classifier-free guidance at inference time to push the denoising process even more strongly towards the conditioning $y$, which for me, is my new `brian_wildsmith` pseudo-word.


With some theory under our belts, let's end with one more example. This is for the prompt 'dove in the style of brian_wildsmith'. In this series of results, the classifier guidance scale ranges from 0 to 30, increasing in increments of $0.02s$, an order of magnitude smaller than in the gifs above. 

<p align="center"> 
<iframe src="https://storage.googleapis.com/hodesdon-com/dove.mp4
" width="512" height="512" frameborder="0" allowfullscreen></iframe>
</p>

There are 50 frames for each $s$-incremement of 1, so 50 frames from the beginning we see the results for $s=1$. This corresponds to the image about 8 seconds in: not discernibly a bird, but just before the turning point at which a bird sharply emerges. This is when the score function is

$$\hat{\epsilon}_{\theta}(x_{t}\vert y) = \epsilon_{\theta}(x_{t}\vert \emptyset) + s(\epsilon_{\theta}(x_{t}\vert y) - \epsilon_{\theta}(x_{t}\vert \emptyset))= \epsilon_{\theta}(x_{t}\vert y)$$
 

That is, at $s=1$ the score function is just the conditional denoising model score function.
Prior to this point, the images resemble a portrait of a woman: clearly not noise, but not an image that is conditioned on the prompt either. 
There is another jump in form 32 seconds in, which is when $s=4$. Beyond this, some more Wildsmith-like detail is added, but in general $s>10$ yields minor variations on the result with no particular improvement in quality or prompt-closeness.

# Textual inversion for bias reduction
Finally, I wanted to flag a use case of textual inversion that I haven't seen discussed beyond the original paper. It is well known that generative models reproduce the biases of their training data. For instance, the prompts 'doctor' and 'scientist' disproportionately produce images that resemble men. This reflects biases in the kind of images that are uploaded to image hosting sites where the training data is collected from.

When DALL-E2 was first released, this bias recieved a lot of attention and OpenAI [scrambled to release a patch](https://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2/) that would force the model to produce more diverse images.
The first pass solution was extremely hacky. It consisted of silently adding a keyword from a list of minority groups (such as 'female', 'black') to the  prompt, as [users quickly discovered](https://twitter.com/rzhang88/status/1549472829304741888) by prompting the model with "A doctor holding a sign that says", and observing the keyword printed on the sign in the generated image. This phenomenon isn't reproducible in current versions, so clearly OpenAI have a more robust solution now. A far better solution would be to fix the training set. But retraining is computationally expensive.

As the authors of the textual inversion paper suggest, we can use the technique to update a trained model's understanding of words already in its vocabulary. Rather than using a novel token, as we do when adding a style or object to the model's vocabulary, we overwrite the embedding of an existing token, training on a small, curated set of images. 

The compute budget to do this is negligible relative to training the base model. My textual inversion above ran in about 4 hours on a GeForce GTX 1080, whereas the initial training of stable diffusion was 150,000 GPU hours on the considerably more FLOP-heavy A100. So this seems like an elegant and practical tool in the toolbelt to use in countering bias.

##References

**Ho, J. and Salimans, T.** *Classifier-free diffusion guidance*, 
In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, preprint https://arxiv.org/abs/2207.12598 2021


**Gal, R, Alaluf Y., Atzmon Y., Patashnik O., Bermano A, Chechik G., and Cohen-Or D.**,
*An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion*,  preprint https://arxiv.org/abs/2208.01618 2022

**Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,  Sutskever, I., Chen, M.**,
*GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models*, preprint https://arxiv.org/abs/2112.10741 2022
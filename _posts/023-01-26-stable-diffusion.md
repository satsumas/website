---
title: 'Personalised style transfer with Stable Diffusion'
date: 2023-01-26 00:00:00
description: Generating images with AI
featured_image: '/images/demo/demo-landscape.jpg'
---

The internet is awash with images generated by AI models like Midjourney, DALLE, and Stable Diffusion. But the images that accompany articles on it tend to be predictably same-y. Sci-fi themes, Blade Runner dystopian cities, anime. 'Photorealistic' images of women that look like the result of heavy filtering.

I wanted to experiment with generating some different looking images using a form of style transfer. I did this using textual inversion on Stable Diffusion.

For this experiment, I wanted to push the model into recreating the textures, palettes and ornate style of children's illustrator Brian Wildsmith. Wildsmith made beautiful books and posters using a range of media, from scratchy pen and ink to rich gauche. One of the most striking things about his work is the use of colour -- fuscias, emerald greens, and frequent use of geometic patterns. I wanted to see how well the model would learn and apply these themes. This isn't the simplest way to showcase stylee transfer, since Wildsmith's style is very broad, spanning a lifetime of work.

See below for a deeper look at the textual inversion and its potential for use in adjusting generative AI trained on biased datasets.

Here are some examples of original Wildsmith:


<!-- images--> 

<style>
.gallery {
  width: 800px;
}

figcaption {
  display: block;
  margin-top: 1em;
  font-style: italic;
  text-align: center;
}


</style>

<figure>
    <div class="gallery" data-columns="1">
        <img src="/images/stable_diffusion/Robert-Louis-Stevenson-a-childs-garden-of-verses-Escape-at-Bedtime-Brian-Wildsmith.jpg">
        <img src="/images/stable_diffusion/Jungle-party-original-illustration-P1&2-brian-wildsmith.jpg">
        <img src="/images/stable_diffusion/burning-sun.jpg">
        <img src="/images/stable_diffusion/sun-closeup.jpg">
        <img src="/images/stable_diffusion/sleeping-beauty-poster-by-brian-wildsmith-for-franklin-watts-NYC.jpg">
    </div>

<figcaption>Original images by Brian Wildsmith</figcaption>
</figure>

# Caveats
I don't want to start throwing around derivative images without saying something about the relationship of style-transfer results to the original creations. Its clear that these results are dependent in a sense on the original artist, and more broadly, I believe that there is a fundamental difference between art (construed broadly, and including among a huge number of other crafts, illustration, photography, and digital art) and AI generated images. Art requires creativity, and constitutes a form of communication, which is not what an latent diffusion algorithm does in outputting images.

Consider the minor scandal at the end of last year when a digital art prize was won by an entry generated by Midjourney. The winner [gloated](https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html) "Art is dead, dude. Itâ€™s over. A.I. won. Humans lost."  I don't see how you can hold this view without knowing both very little about the technology and very little about art. And I am pretty sure that profesional artists aren't that worried about AI coming for their jobs.

Art is just a minority of the professions that create graphics, and it seems inevitable that some of this wider work will end up being automated by generative AI. However there is something intrinsic to art that makes it impossible to automate.

Enough about art, back to the tech.

## Textual inversion on a style
There is a lot of interest in fine-tuning large foundation models to make them applicible to custom domains. This doesn't work so well in practice, due amongst other things to the alarmingly named 'catastrophic forgetting'. But there are reliable methods that can be used to add specific representational capacity to a pretrained model, such as textual inversion.

Textual inversion enables you to add a new pseudo-word to the embedding space of the model, representing a chosen image or style. The method effectively teaches the model the new word by examples.

Gal et al 2022 present textual inversion on text-to-image models. They suggest 3-5 images of an object is sufficient to embed it into the model. I started off with about 6 images in my training set, and then increased this considerably, first to 136 and ultimately to 318.

## Data and setup
I used [Automatic1111's webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui) to train the textual inversion on Stable Diffusion v1.4.

My dataset was around 300 images I scanned from Brian Wildsmith books and found online. I named each file with a brief description of the content. The filenames are used in training to generate prompts of the form `"a painting of [filewords] by [name]"`, where `[filewords]` come from the filename and `[name]` is the new pseudo-word.


Images were cropped to 512x512. One hurdle was how to programmatically crop the images in a way that preserved features of interest. Centre-cropping meant that the resulting models generated images missing the tops of heads. For speed, I used a feature of the webui to programatically crop [based on detected focal point](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/3139) in combination with splitting large images into smaller, overlapping tiles, but this was not entirely satisfactory.


To generate the embedding, I used 16 vectors per token rather than the default of 1, because there are lots of different aspects of the training set I wanted to capture and represent with my new token (use of geometric patterns, textures, colour combinations etc). Increasing the value yielded improvements in texture, for instance, enabling the representation of brushstrokes, as shown in examples below.


The textual inversion implementation in Gal et al uses an initialisation text, which is set to a "single word coarse descriptor" of the new object being introduced. For instance, to add a particular dog to the model, initialise with "dog". In the case of style transfer, it wasn't clear what word to use, since the images in the training set are only related by style and not by subject matter. I went with `illustration`. 

## Results
Here's a couple of images that were generated randomly during training of the new embedding. 

<style>
.gallery {
  width: 1000px;
}
</style>
<div class="gallery" data-columns="2">
    <img src="/images/stable_diffusion/generated/owl_7000steps.png">
    <img src="/images/stable_diffusion/generated/elephant_9000steps.png">
</div>

The geometric pattern in the background of the elephant is frequently generated by my model. It appears in the training set in some of Wildsmith's illustrations of abstract shapes. For instance, from his counting book:


<style>
.gallery {
  width: 1000px;
}
</style>

<figure>
<div class="gallery" data-columns="3">
    <img src="/images/stable_diffusion/number-1-from-brian-wildsmiths-123.png">
    <img src="/images/stable_diffusion/number-2-from-brian-wildsmiths-123.png">
    <img src="/images/stable_diffusion/number-three-from-brian-wildsmiths-123.png">
</div>

<figcaption>Original images from 123 by Brian Wildsmith (1970)</figcaption>
</figure>

The decorative design on the elephant's trunk does not appear in the training set, but was created by the model. The closest thing in the training set, in terms of content, is the following elephant with a decorative saddle. The generated image is clearly different and draws from different aspects of the Wildsmith style:


<figure>
<p align="center"> 
<img src="/images/stable_diffusion/elephant-with-decorative-clothes.png" width="300">
</p>
<figcaption>From What the Moon Saw by Brian Wildsmith</figcaption>
</figure>


The following sets of examples were created using the same prompts, with embeddings using 1 and 16 vectors per token respectively, though different random seeds in each case. The version with 16 vectors per token captures more of the texture of the pencil and paintbrush strokes in the original images, as well as some unintended artefacts of the training data like page creases, photographic glare and shadows.

Results were generated with 20-80 sampling steps. I raised CFG scale to 14 from the default 7. Theoretically this should force closer adherence to the prompt; I found it gave better looking results, although it also tended to reproduce more of the unwanted artefacts too.

<figure>
<div class="gallery" data-columns="3">
    <img src="/images/stable_diffusion/generated/00053-2402323073-colourful-oil-painting-of-a-cat-in-the-style-of-brian_wildsmith.png">
    <img src="/images/stable_diffusion/generated/00062-3942776645-a-magnificent-multicoloured-sun.png">
    <img src="/images/stable_diffusion/generated/1_vector_00151-64173078-a decorative-cottage-in-forest-surrounded-by- green-trees-and-flowers-the-style-of-zzl_brian_wildsmith_318_noresize_preproce.png">
</div>
</figure>
Using 16 vectors per token (sadly the cat image was a victim of the cropping issue):

<figure>
<div class="gallery" data-columns="3">
    <img src="/images/stable_diffusion/generated/16vectors_00135-1247160374-a colourful oil painting-of-a-cat-in-the-style-of-brian_wildsmith.png">
    <img src="/images/stable_diffusion/generated/16vectors_00137-1068457987-a-magnificent-multicoloured-sun-in-the-style-of-brian_wildsmith.png">
    <img src="/images/stable_diffusion/generated/16vectors_00153-1093123542-a decorative-cottage-in-forest-surrounded-by-green-trees-and-flowers-in-style-of-jwll_wildsmith.png">
</div>
<figcaption>Images generated by Stable Diffusion with textual inversion</figcaption>
</figure>



## Classifier-free guidance for style transfer
One parameter I experimented with was the value controlling classifier free guidance. Classifier free guidance is a technique for varying the closeness with which the generated results adhere to the prompt. I wanted to understand what this means in the context of style transfer. In Automatic1111's web UI, this parameter is `cfg_scale`.

To show the effect on varying `cfg_scale`, see the sequence of images below, for `cfg_scale` ranging from 0 and 30 with a fixed seed. The model was prompted to draw a city of tents in the desert under a full moon, in Wildsmith's style. I think this reveals that `cfg_scale` isn't having all that much effect on the quality of the generated image, once it gets past about 1, except for a few mid-range values where it fails to render the moon. Otherwise, it's just gently exploring a set of similar points in the space.


<div class="gallery" data-columns="1">
    <img src="https://storage.googleapis.com/hodesdon-com/tent_city.gif">
</div>

This time, a house with a mountain range in the background. The image flickers between different placements of the house, leaving it out altogether for large `cfg_scale` values.

<div class="gallery" data-columns="1">
    <img src="https://storage.googleapis.com/hodesdon-com/mountain_range.gif">
</div>

I'll briefly recap what classifier free guidance does. Let's begin with classifier guidance, introduced by Dhariwal and Nichol (2021) as the method that gave diffusion models the edge over GANs in image synthesis. I found [Beanne's blog post](https://benanne.github.io/2022/05/26/guidance.html) on guidance gave a careful motivation for the methods, and  [Lilian Weng's blog post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#reverse-diffusion-process) covered the maths. This post builds on their accounts by including concrete examples of varying the classifier free guidance values.

To see what classifier guidance does, let's recap what a conditional diffusion model like stable diffusion does. The trick to understanding the model is to see that what we are modeling -- images -- are sampled from a probability distribution $q(x)$, albeit an extremely complex one. Images are not noise, as they have a structure, which can be represented by a distribution. We can also consider a noising process, called forward diffusion, in which we add incremental amounts of Gaussian noise to an image $x_{0}$ drawn from $q(x)$, giving us a sequence $x_{t}$ for $t\in[0, T]$ such that $x_{T}$ is pure noise. If we had access to $q$, we could take a sample of pure noise and apply reverse diffusion, denoising it back to content, by estimating $q(x_{t}\vert x_{t+1})$. This is not possible, however, since $q$ depends on the entire dataset of images and is therefore intractable to compute.


What we can do is to train a model that approximates these conditional probabilities $q(x_{t}\vert x_{t+1})$.. Call this model $p_{\theta}(x_{t} \vert x_{t+1})$. Under certain assumptions, the distribution $p_{\theta}$ is a Gaussian. There is an additional detail: we're considering _conditional_ diffusion models, which is to say, denoising is conditioned on some text prompt. We don't just want our denoising process to reveal some image in the underlying distribution $q(x)$, but rather to reveal the image most closely aligned with a given text prompt, $y$. This means we are in fact dealing with the conditional probability $p_{\theta}(x_{t}\vert y,t)$, which can be also approximated by a Gaussian. In practice, diffusion models approximate $\nabla_{x}log(p_{\theta}(x \vert y))$, the 'score function' of the distribution, rather than the distribution itself. 

Distribution $p_{\theta}$ gives rise to an associated denoising model $\epsilon_{\theta}(x_{t}, y)$, which predicts the noise at timestep $t$ given the prompt $y$ and the image $x_{t}$, which is equivalent to predicting the incrementally denoised image $x_{t-1}$.

### Conditional reverse diffusion
Sampling from a conditional denoising model is possible given an additional classifier model trained on noisy versions of $x$ and grouped into classes $y$. Dhariwal and Nichol, reason as follows: suppose first that we have a conditional Markovian noising process $\widehat{q}$, such that $\widehat{q}(y|x_{0})$ is a known label distribution for each sample. If this sounds familiar, it should: it's exactly what you would get out of a softmax function at the end of a regular classifier model over classes $y$. Dhariwal and Nichol establish that $\widehat{q}(y|x_{t+1} \vert x_{t}) = \widehat{q}(y|x_{t+1} \vert x_{t}, y)$, which is just to say that the distribution doesn't change whether or not it is conditioned on $y$. And they also prove, using the fact that $\widehat{q}(y|x_{0}) = q(y|x_{0})$, that $\widehat{q}(y|x_{t}) = q(y|x_{t})$. For the full derivation see Appendix H of the paper. Using these identities, and repeated applications of the multiplucation rule of probability, they can then establish that:

$\widehat{q}(y \vert x_{t+1}, y ,x_{t}) = \frac{\widehat{q}(x_{t}, x_{t+1}, y)}{\widehat{q}(x_{t+1}, y)}$

$= \frac{\widehat{q}(x_{t}, x_{t+1}, y)}{\widehat{q}(y \vert x_{t+1}) \widehat{q}(x_{t+1})}$

$ = \frac{\widehat{q}(x_{t} \vert x_{t+1})  \widehat{q}(y \vert x_{t}, x_{t+1})  \widehat{q}(x_{t+1})} {\widehat{q}(y \vert x_{t+1}) \widehat{q}(x_{t+1})}$ 

$= \frac{\widehat{q}(x_{t} \vert x_{t+1})  \widehat{q}(y \vert x_{t}) } {\widehat{q}(y \vert x_{t+1})}$

$= \frac{q(x_{t} \vert x_{t+1})  \widehat{q}(y \vert x_{t}) } {\widehat{q}(y \vert x_{t+1})}$ by previously established identity

$= Z q(x_{t} \vert x_{t+1})  \widehat{q}(y \vert x_{t})$ 

Since $\widehat{q}(y \vert x_{t+1}$ does not depend on $x_t$, we can treat it as a constant $\frac{1}{Z}$.

Since $p_{\theta}(x_{t} \vert x_{t+1})$ approximates $q(x_{t} \vert x_{t+1})$, we have

$\widehat{q}(y \vert x_{t+1}, y ,x_{t}) \approx p_{\theta}(x_{t} \vert x_{t+1}) \widehat{q}(y \vert x_{t})$. 

And we can, as mentioned above, easily train a classifier to give us $\widehat{q}(y \vert x_{t})$. As we can therefore compute both factors of the left hand side, we can sample from the conditional reverse diffusion process $\widehat{q}(y \vert x_{t+1}, y ,x_{t})$.

OK, so that is an overview of how conditional diffusion models work. For the rest of the section we'll follow the notational convention of suppressing the timestep annotations, and assume for each timestep $t$ a function $p_{\theta}(x\vert y) = p_{\theta}(x_{t}\vert y,t)$

The insight of classifier guidance is that we can modify the score function of the noise predictor $\epsilon_{\theta}$ in such a way that the denoised images produced at inference time are more closely aligned to the prompt. This works by introducing a scalar coefficient controlling one factor (the classifier term) of the score function.

In simple terms, in reverse conditional diffusion, we want to know how much the log-probability of an image $x$ given prompt $y$ changes when we change $x$, so we can iteratively move towards the $x$ that maximises this value. This is the gradient of the log-probability, $ \nabla_{x} log(p_{\theta}(x\vert y))$, which we introduced above as the score function.

We can apply Bayes rule, below, to rewrite the score function as follows: 

Bayes rule entails that $p_{\theta}(x\vert y) = \frac{p_{\theta}(y\vert x).p_{\theta}(x)}{p_{\theta}(y)}$


Then, taking gradients w.r.t. $x$, and distributing the log:

$\Rightarrow\nabla_{x}log(p_{\theta}(x\vert y)) = \nabla_{x}log \frac{p_{\theta}(y\vert x)p_{\theta}(x)}{p_{\theta}(y)}$

$\Rightarrow \nabla_{x} log(p_{\theta}(x\vert y)) = \nabla_{x}log(p_{\theta}(y\vert x)) + \nabla_{x} log(p_{\theta}(x)) - \nabla_{x} log(p_{\theta}(y))$

$\Rightarrow \nabla_{x} log(p_{\theta}(x\vert y)) = \nabla_{x}log (p_{\theta}(y\vert x)) + \nabla_{x} log(p_{\theta}(x))   \space \space\space \space\space \space (*)$ 

Since $p_{\theta}(y)$ is not a function in x, its gradient w.r.t. $x$ is 0.

Notice that via the rearrangement above, we are expressing the gradient of the conditional probability $log(p_{\theta}(x\vert y))$ in terms of the gradients of the _unconditional probability_ $log(p_{\theta}(x))$ and the _classifier_ $log(p_{\theta}(y\vert x))$. It is a classifier because it is just the thing that a vanilla classification model learns: the probability of the label given the data. In this way, the diffusion model's score function can be decomposed into an unconditional probability and a classifier.

Classifier guidance tweaks the score function, to introduce a coefficient $s$ controlling the classifier term $\nabla_{x}log (p_{\theta}(y\vert x))$.

So, what is the effect on our model outputs of varying $s$ in the new score function? Well, we know that $s\nabla_{x}log (p_{\theta}(y\vert x)) = \nabla_{x} log \frac{1}{Z}p_{\theta}(y\vert x)^{s}$ for a constant $Z$. This is to say, multiplying the log probability of $y$ given $x$ by $s$ is proportional to raising the renormalised log probability to the power $s$. And this exponentiation has the effect of disproportionately increasing its larger values, amplifying the modes of the distribution, which are the values that maximise the probability density function. So, for $s>1$, the $x$ that maximises $s\nabla_{x}log (p_{\theta}(y\vert x)) + \nabla_{x} log(p_{\theta}(x))$ will be closer to the mode of the distribution $p_{\theta}(y\vert x)$, so closer to the class label $y$.

This is why classifier guidance is known to have the effect of boosting the fidelity of diffusion model outputs at the expense of diversity. As the name classifier guidance suggests, it is used to guide the score function closer to the modes of a classifier based on the prompt. Note that in order to apply the modified score function in your model, you need a classifier trained on noisy images to predict classes, typically the Imagenet classes.

Classifier free guidance (Ho & Salimans, 2021) is a development of classifier guidance. It pushes the model in the same direction as classifier guidance, but avoids the need to train a specialised classifier.

To use  classifier-free guidance, during training we replace the label $y$ in a conditonal diffusion model with a null label, $\emptyset$, a fixed proportion of the time, typically 10-20%. Recall that the de-noising process is modeled by $\epsilon_{\theta}(x_{t}\vert y)$. We replace this with $\widehat{\epsilon}_{\theta}(x_t \vert y)$, a weighted combination of the original conditional denoising model and an unconditional denoising model as follows:


$$\widehat{\epsilon}_{\theta}(x_{t}\vert y) = \epsilon_{\theta}(x_{t}\vert \emptyset) + s(\epsilon_{\theta}(x_{t}\vert y) - \epsilon_{\theta}(x_{t}\vert \emptyset))$$


For $s=0$, $\widehat{\epsilon}_{\theta}$ is just the unconditional denoising model, and for $s=1$, it is the original conditional denoising model. But as $s>1$ increases, the model is a mixture of the conditional and unconditional, increasingly weighted towards the conditional model.


The version of stable diffusion I used, `v1.4`, was trained with [10% text-conditioning dropout](https://huggingface.co/CompVis/stable-diffusion-v1-4), allowing me to use classifier-free guidance at inference time to push the denoising process even more strongly towards the conditioning $y$, which for me, is my new `brian_wildsmith` pseudo-word.


Now we have some theory under our belts, let's end with one more example. This is for the prompt 'dove in the style of brian_wildsmith'. In this series of results, the classifier guidance scale ranges from 0 to 30, increasing in increments of $0.02s$, an order of magnitude smaller than in the gifs above. 

<p align="center"> 
<iframe src="https://storage.googleapis.com/hodesdon-com/dove.mp4
" width="512" height="512" frameborder="0" allowfullscreen></iframe>
</p>

There are 50 frames for each $s$-incremement of 1, so 50 frames from the beginning we see the results for $s=1$. This corresponds to the image about 8 seconds in: not discernibly a bird, but just before the turning point at which a bird sharply emerges. This is when the score function is

$$\widehat{\epsilon}_{\theta}(x_{t}\vert y) = \epsilon_{\theta}(x_{t}\vert \emptyset) + s(\epsilon_{\theta}(x_{t}\vert y) - \epsilon_{\theta}(x_{t}\vert \emptyset))= \epsilon_{\theta}(x_{t}\vert y)$$
 

That is, at $s=1$ the score function is just the conditional denoising model score function.
Prior to this point, the images resemble a portrait of a woman: clearly not noise, but not an image that is conditioned on the prompt either. 
There is another jump in form 32 seconds in, which is when $s=4$. Beyond this, some more Wildsmith-like detail is added, but in general $s>10$ yields minor variations on the result with no particular improvement in quality or prompt-closeness.

# Textual inversion for bias reduction
Finally, I wanted to flag a use case of textual inversion that I haven't seen discussed beyond the original paper. It is well known that generative models reproduce the biases of their training data. For instance, the prompts 'doctor' and 'scientist' disproportionately produce images that resemble men. This reflects biases in the kind of images that are uploaded to image hosting sites where the training data is collected from.

When DALL-E2 was first released, this bias recieved a lot of attention and OpenAI [scrambled to release a patch](https://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2/) that would force the model to produce more diverse images.
The first pass solution was extremely hacky. It consisted of silently adding a keyword from a list of minority groups (such as 'female', 'black') to the  prompt, as [users quickly discovered](https://twitter.com/rzhang88/status/1549472829304741888) by prompting the model with "A doctor holding a sign that says", and observing the keyword printed on the sign in the generated image. This phenomenon isn't reproducible in current versions, so clearly OpenAI have a more robust solution now. A far better solution would be to fix the training set. But retraining is computationally expensive.

As the authors of the textual inversion paper suggest, we can use the technique to update a trained model's understanding of words already in its vocabulary. Rather than using a novel token, as we do when adding a style or object to the model's vocabulary, we overwrite the embedding of an existing token, training on a small, curated set of images. 

The compute budget to do this is negligible relative to training the base model. My textual inversion above ran in about 4 hours on a GeForce GTX 1080, whereas the initial training of stable diffusion was 150,000 GPU hours on the considerably more FLOP-heavy A100. So this seems like an elegant and practical tool in the toolbelt to use in countering bias.

## References

**Ho, J. and Salimans, T.** *Classifier-free diffusion guidance*, 
In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, preprint https://arxiv.org/abs/2207.12598 2021


**Gal, R, Alaluf Y., Atzmon Y., Patashnik O., Bermano A, Chechik G., and Cohen-Or D.**,
*An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion*,  preprint https://arxiv.org/abs/2208.01618 2022

**Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,  Sutskever, I., Chen, M.**,
*GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models*, preprint https://arxiv.org/abs/2112.10741 2022
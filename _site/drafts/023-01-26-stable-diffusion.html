<!doctype html>

<html class="no-js" lang="en">

<head>


	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

	Kate Hodesdon

	Duet Theme by https://jekyllthemes.io
	Premium + free Jekyll themes for your blog or website.

	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->


	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

	<!-- Page Info -->
	<link rel="shortcut icon" href="/images/favicon.png">
	<title>Personalised style transfer with Stable Diffusion – Kate Hodesdon</title>
	<meta name="description" content="Generating images with AI">

	<!-- Twitter Card -->
	<meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:title" content="Personalised style transfer with Stable Diffusion – Kate Hodesdon">
	<meta name="twitter:description" content="Generating images with AI">
	<meta name="twitter:image:src" content="http://localhost:4000/images/demo/demo-landscape.jpg">

	<!-- Facebook OpenGraph -->
	<meta property="og:title" content="Personalised style transfer with Stable Diffusion – Kate Hodesdon" />
	<meta property="og:description" content="Generating images with AI" />
	<meta property="og:image" content="http://localhost:4000/images/demo/demo-landscape.jpg" />

	
	<!-- Font Embed Code -->
	<link href="https://fonts.googleapis.com/css?family=Muli:300,400,600,700" rel="stylesheet">
	

	<!-- Styles -->
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" href="/css/style.css">
	
	<!-- Icons -->
	<script defer src="https://use.fontawesome.com/releases/v5.1.1/js/solid.js" integrity="sha384-GXi56ipjsBwAe6v5X4xSrVNXGOmpdJYZEEh/0/GqJ3JTHsfDsF8v0YQvZCJYAiGu" crossorigin="anonymous"></script>
	<script defer src="https://use.fontawesome.com/releases/v5.1.1/js/brands.js" integrity="sha384-0inRy4HkP0hJ038ZyfQ4vLl+F4POKbqnaUB6ewmU4dWP0ki8Q27A0VFiVRIpscvL" crossorigin="anonymous"></script>
	<script defer src="https://use.fontawesome.com/releases/v5.1.1/js/fontawesome.js" integrity="sha384-NY6PHjYLP2f+gL3uaVfqUZImmw71ArL9+Roi9o+I4+RBqArA2CfW1sJ1wkABFfPe" crossorigin="anonymous"></script>

	
	<!-- Custom Styles -->
	<style></style>
	

	
	<!-- Analytics Code -->
	
	

	
	<!-- Extra Header JS Code -->
	
	
	
</head>


<body class="loading ajax-loading" data-site-url="http://localhost:4000" data-page-url="/drafts/023-01-26-stable-diffusion.html">


	<header class="header">

	<div class="wrap">

		
		<a href="/blog/" class="header__title">
			Kate Hodesdon
		</a>
		

		<div class="menu">
			<div class="menu__toggle js-menu-toggle">
				<div class="menu__toggle__icon"><span></span></div>
			</div>
			<div class="menu__wrap">
				<ul class="menu__list">
					
					<li class="menu__list__item">
						<a href="/blog/" class="menu__list__item__link">Blog</a>
					</li>
					
					<li class="menu__list__item">
						<a href="/about" class="menu__list__item__link">About</a>
					</li>
					
					<li class="menu__list__item">
						<a href="/contact" class="menu__list__item__link">Contact</a>
					</li>
					
				</ul>
			</div>
		</div>

	</div>

</header>


	<div class="loader"><svg width="120" height="30" viewBox="0 0 120 30" xmlns="http://www.w3.org/2000/svg"><circle cx="15" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite" /><animate attributeName="fill-opacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite" /></circle><circle cx="60" cy="15" r="9" fill-opacity="0.3"><animate attributeName="r" from="9" to="9" begin="0s" dur="0.8s" values="9;15;9" calcMode="linear" repeatCount="indefinite" /><animate attributeName="fill-opacity" from="0.5" to="0.5" begin="0s" dur="0.8s" values=".5;1;.5" calcMode="linear" repeatCount="indefinite" /></circle><circle cx="105" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite" /><animate attributeName="fill-opacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite" /></circle></svg></div>

	<div class="page-loader"></div>

	
	<div class="page">

		<div class="page__content" data-page-title="Personalised style transfer with Stable Diffusion – Kate Hodesdon">

			<section class="intro">

	<div class="wrap">

		<h1>Personalised style transfer with Stable Diffusion</h1>
		<p></p>

	</div>

</section>

<section class="single">

	<img src="/images/stable_diffusion/fruit-and-star-heart-Brian-Wildsmith.jpg" width="300" alt="Image by Brian Wildsmith">


## Generative AI

The internet is awash with images generated by AI models like Midjourney, DALLE, and Stable Diffusion. But, the demo images that accompany articles on it tend to be a bit stylistically same-y. Sci-fi themes, Blade Runner-y cities, anime. 'Photorealistic' images of women that have the facetuning built in.

I wanted to experiment with generating some different looking images using a form of style transfer. I did this using textual inversion on Stable Diffusion.

For experiment 1, I wanted to push the model into recreating the textures, palettes and ornate style of children's illustrator Brian Wildsmith. Wildsmith made beautiful books and posters using a range of media, from scratchy pen and ink to rich gauche. One of the most striking things about his work is the use of colour! Rich fuscias, emerald greens. Frequent use of geometic patterns. I wanted to see how well the model would learn and apply these themes. This would be a difficult task, since Wildsmith's style is very broad, spanning a lifetime of work.

Here are some examples of original Wildsmith:


<!-- images--> 

<style>
.gallery {
  width: 800px;
}

figcaption {
  display: block;
  margin-top: 1em;
  font-style: italic;
  text-align: center;
}


</style>

<figure>
    <div class="gallery" data-columns="1">
        <img src="/images/stable_diffusion/Robert-Louis-Stevenson-a-childs-garden-of-verses-Escape-at-Bedtime-Brian-Wildsmith.jpg">
        <img src="/images/stable_diffusion/Jungle-party-original-illustration-P1&2-brian-wildsmith.jpg">
        <img src="/images/stable_diffusion/burning-sun.jpg">
        <img src="/images/stable_diffusion/sun-closeup.jpg">
        <img src="/images/stable_diffusion/sleeping-beauty-poster-by-brian-wildsmith-for-franklin-watts-NYC.jpg">
    </div>

<figcaption>Original images by Brian Wildsmith</figcaption>
</figure>

# Is any of this OK?
Before I start throwing around derivative images from a model trained on Brian Wildsmith's work, I should address a frequently voiced concern with so-called AI art. There are many ethical and normative questions that have been raised concerning AI Art. A significant worry is that AI is plagiarising artists. Another is that it is coming for their jobs.


I want to make it clear that the AI-generated images trained on Wildsmith's work don't hold a candle to the real thing, in terms of ingenuity, creativity, or the simple human communication that is essential to art. I don't see any evidence that AI is replacing artists. But, of course, art represents only a tiny fraction of professions that generate graphics. Commercial photographers and graphic designers, for instance, could automate some of their output with generative AI, and we should be realistic about the fact that the number of people required to do these roles will decline. However, art -- construed broadly, and including illustration -- is very different from those other activities, and it has always puzzled me why people see generative AI as replacing _artists_, and not, say, commercial photographers or graphic designers. I am construing art broadly, to include illustration, as well as photography and digital art. Like many concepts, there is no easy demarcation; instead there is a spectrum, from fine art, through decorative art and design, to functional images like icons or stock photography. But it is the stock photography end of the spectrum that is ripe for disruption with generative AI, while the pure art end is less so.

The reason for art's protection from automation is that there is something intrinsically human about art. Art is produced with intent. In many cases, our appreciation of a piece of art consists in our recognition of a mood, or outlook, that the artist conveys. Part of the artistic value of an Edward Hopper painting is the urban lonliness it captures. Were the same image to be produced by a random process, with no agent behind it with an aim of communication, then it would not have the same value. Natural objects are produced by the agent-less process of evolution. No matter how beautiful it is, no sunset, shell or flower is _art_.
What we have with text2img models is something half-way between: while the AI is responsible for a lot of the work, there is a human behind the images, guiding the generative process with words, with which the person is communicating something.

It will be interesting to see the truly original moves that AI makes in image generation, analogous to those made in the game of Go by alphago.

Besides a vocal minority of digital artists, I doubt that artists are all that worried about being replaced by AI.


Even independently of fine art photography, if we consider jounralistic photography, it is clear that here as well there are differences in value between human-generated and AI-generated images. Consider a stock photograph, maybe a woman laughing alone with salad. Do we care if she exists? Not especially. The opposite is true of iconic photographs such as Tank Man, the protestor standing in front of a column of tanks in Tiananmen square, or Dorothea Lange’s 'Migrant Mother', the dustbowl refugee family. These photographs have cultural value that a purely synthetic image, no matter how realistic, can never have. It's not just the case that the photographs are a record of a historically significant event, but also that the photographer has, with the photograph, captured and communicated a particularly poignant emotion.



Consider language models, by analogy. Nobody expects ChatGPT to replace poets or novelists, though it might churn out limericks, some of which are funny, or even formulaic stories for children. But the low hanging fruit is the generation of functional text, such as some marketing copy, or mid-tier undergraduate essays, which just need to tick a few boxes. Really, language models are better at synthesising text given a learnt corpus than they are at generating completely novel ideas, which is why applications like chatGPT, which do question answering, as so effective. 

There was a minor scandal at the end of last year when a digital art prize was won by an entry generated by Midjourney. The person who submitted the piece [gloated](https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html) "Art is dead, dude. It’s over. A.I. won. Humans lost."  But AI tools are not opposed to human creators in a battle for dominance. To hold this view, you either need to know both very little about the technology and very little about art.

That said, back to the tech.

## Textual inversion on a style
Textual inversion is a technique to add representational capacity to a pretrained text to image model. It does this by adding a new pseudo-word to the embedding space of the model, representing a new visual pattern, which could be an image of a specific object or a style, as in my use case. It effectively teaches the model a new word by examples.

The paper on textual inversion (Gal et al, 2022) suggests 3-5 images of an object is sufficient to embed it into the model. I started off with about 6 images in my training set, and then increased this considerably, first to 136 and ultimately to 318.

## Data and setup
I used Automatic1111's webui to train the textual inversion, and it's txt2img function to generate inferences.

My dataset was around 300 images I scanned from Brian Wildsmith books and found online. I named each file with a brief description of the content. The filenames are used in training to generate prompts of the form `"a painting of [filewords] by [name]"`, where `[filewords]` come from the filename and `[name]` is the new pseudo-word.


Images were cropped to 512x512. One hurdle was how to programmatically crop the images in a way that preserved features of interest. Centre-copping meant that the resulting models generated images missing the tops of heads. To get around this, I used a feature of the webui to crop [based on detected focal point](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/3139) in combination with splitting large images into smaller, overlapping tiles, but this was not entirely satisfactory.


To generate the embedding, I used 16 vectors per token rather than the default of 1, because there are lots of different aspects of the training set I wanted to capture and represent with my new token (use of geometric patterns, textures, colour combinations etc). Increasing the value yielded improvements in texture, for instance, enabling the representation of brushstrokes, as shown in examples below.


The textual inversion implementation in Gal et al uses an initialisation text, which is set to a "single word coarse descriptor" of the new object being introduced. For instance, to add a particular dog to the model, initialise with "dog". In the case of style transfer, it wasn't clear what word to use, since the images in the training set are only related by style and not by subject matter. I went with `illustration`. 

## Results
Here's a couple of images that were generated randomly during training of the new embedding. 

<style>
.gallery {
  width: 1000px;
}
</style>
<div class="gallery" data-columns="2">
    <img src="/images/stable_diffusion/generated/owl_7000steps.png">
    <img src="/images/stable_diffusion/generated/elephant_9000steps.png">
</div>

The geometric pattern in the background of the elephant is frequently generated by my model. It appears in the training set in some of Wildsmith's illustrations of abstract shapes. For instance, from his counting book:


<style>
.gallery {
  width: 1000px;
}
</style>

<figure>
<div class="gallery" data-columns="3">
    <img src="/images/stable_diffusion/number-1-from-brian-wildsmiths-123.png">
    <img src="/images/stable_diffusion/number-2-from-brian-wildsmiths-123.png">
    <img src="/images/stable_diffusion/number-three-from-brian-wildsmiths-123.png">
</div>

<figcaption>Original images from 123 by Brian Wildsmith (1970)</figcaption>
</figure>

The decorative design on the elephant's trunk does not appear in the training set, but was created by the model. The closest thing in the training set, in terms of content, is the following elephant with a decorative saddle. The generated image is clearly different and draws from diffrent aspects of the Wildsmith style:


</style>
<img src="/images/stable_diffusion/elephant-with-decorative-clothes.png" width="300">


The following sets of examples were created using the same prompts, with embeddings using 1 and 16 vectors per token respectively, though different random seeds in each case. The version with 16 vectors per token captures more of the texture of the pencil and paintbrush strokes in the original images, as well as some unintended artefacts of the training data like page creases, photographic glare and shadows.

Results were generated with 20-80 sampling steps. I raised CFG scale to 14 from the default 7. Theoretically this should force closer adherence to the prompt; I found it gave better looking results, although it also tended to reproduce more of the unwanted artefacts too.

<div class="gallery" data-columns="3">
    <img src="/images/stable_diffusion/generated/00053-2402323073-colourful-oil-painting-of-a-cat-in-the-style-of-brian_wildsmith.png">
    <img src="/images/stable_diffusion/generated/00062-3942776645-a-magnificent-multicoloured-sun.png">
    <img src="/images/stable_diffusion/generated/1_vector_00151-64173078-a decorative-cottage-in-forest-surrounded-by- green-trees-and-flowers-the-style-of-zzl_brian_wildsmith_318_noresize_preproce.png">
</div>

Using 16 vectors per token (sadly the cat image was a victim of the cropping issue):

<div class="gallery" data-columns="3">
    <img src="/images/stable_diffusion/generated/16vectors_00135-1247160374-a colourful oil painting-of-a-cat-in-the-style-of-brian_wildsmith.png">
    <img src="/images/stable_diffusion/generated/16vectors_00137-1068457987-a-magnificent-multicoloured-sun-in-the-style-of-brian_wildsmith.png">
    <img src="/images/stable_diffusion/generated/16vectors_00153-1093123542-a decorative-cottage-in-forest-surrounded-by-green-trees-and-flowers-in-style-of-jwll_wildsmith.png">
</div>

To show the effect on varying `cfg_scale`, see the sequence of images below, for `cfg_scale` ranging from 0 and 30 with a fixed seed. The model was prompted to draw a city of tents in the desert under a full moon, in Wildsmith's style. I think this reveals that `cfg_scale` isn't having all that much effect on the quality of the generated image, once it gets past about 1, except for a few mid-range values where it fails to render the moon. Otherwise, it's just gently exploring a set of similar points in the space.


<div class="gallery" data-columns="1">
    <img src="/images/stable_diffusion/generated/tent_city.gif">
</div>

This time, a house with a mountain range in the background. The image flickers between different placements of the house, leaving it out altogether for large `cfg_scale` values.

<div class="gallery" data-columns="1">
    <img src="/images/stable_diffusion/generated/mountain_range.gif">
</div>


#TODO more smooth example

## Classifier-free guidance for style transfer
One parameter I experimented with was the value controlling classifier free guidance. In Automatic1111's webui, this parameter is called `cfg_scale`. Classifier free guidance is a technique for varying the closeness with which the generated results adhere to the prompt. I wanted to understand what this means in the context of style transfer. 


I'll briefly recap what classifier free guidance does. Let's begin with classifier guidance, introduced by Dhariwal and Nichol (2021) as the method that gave diffusion models the edge over GANs in image synthesis. I found [Beanne's blog post](https://benanne.github.io/2022/05/26/guidance.html) on guidance gave a helpful motivation for both methods.

To see what classifier guidance does, let's recap what a conditional diffusion model like stable diffusion does.

It's easiest to start from inference. At inference time, when you ask your model to produce an image in response to your prompt, the model is running a reverse diffusion process to form an image from noise. The problem is set up as removing the noise in increments: $x_{T}$ is pure noise, $x_{0}$ is noiseless, and for each $t \in [0, T]$, $x_{t-1}$ is slightly less noisy version than $x_{t}$. We are able to do this denoising because the model is trained to denoise images.

It is intractable to compute the probability of $x_{t-1}$ given $x_{t}$ because it depends on the entire dataset. However, we can train a model $\epsilon_{\theta}(x_{t})$ that approximates these conditional probabilities. 

In training, the model is shown random pairs of noised images, $x_{t}$ and timesteps $t$, and it learns predict the noise, $\epsilon_{t}$ which is equivalent to predicting the denoised image $x_{t-1}$ from $x_{t}$ and timestep $t$. This is the denoising model. While $\epsilon_{t}$ is the model, $p_{\theta}(x_{t})$  is the probability distribution of the model's outputs. There is just one additional detail: we're considering _conditional_ diffusion models, which is to say, denoising is conditioned on some text prompt $y$. So we are actually dealing with the conditional probability $p_{\theta}(x_{t}|y, t)$.
Under certain assumptions, the distribution $p_{\theta}$ is a Gaussian. You can read more of the details in [Lilian Weng's blog post on diffusion models](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#reverse-diffusion-process). In the remainder, we'll follow the notational convention of suppressing the timestep annotations, and assume for each timestep $t$ a function $p_{\theta}(x|y) = p_{\theta}(x_{t}|y, t)$


The insight of classifier guidance is that we can modify the sampling process from $p_{\theta}$ in such a way that the resulting image is more closely aligned to the prompt.

The way that this works in practice involves mixing in an additional factor, parameterised by a scalar coefficient. The extra factor is contributed by an additional model, specifially a classifer. It's not a model that is especially hard to train, but it does add an extra overhead in pursuit of higher quality images.

Let's see why a classifier gives us something helpful in sampling from the diffusion model.

In simple terms, we want to know how much the log-probability of an image $x$ given $y$ changes when we change $x$. This is the gradient of the log-probability, $ \nabla_{x} log p_{\theta}(x|y)$. Then we can iteratively move towards such an x.

The score function, which is what is optimised at inference time, [[[IS IT???]]] approximates $ \nabla_{x} log p_{\theta}(x|y)$.


We can apply Bayes rule to rewrite the score function as equation (*) below: 

$p_{\theta}(x|y) = \frac{p_{\theta}(y|x).p_{\theta}(x)}{p_{\theta}(y)} \newline$


Then, taking gradients w.r.t. $x$, and distributing the log:

$\Rightarrow \nabla_{x} log p_{\theta}(x|y) = \nabla_{x} log \frac{p_{\theta}(y|x).p_{\theta}(x)}{p_{\theta}(y)} \newline$
$\Rightarrow \nabla_{x} log p_{\theta}(x|y) = \nabla_{x}log (p_{\theta}(y|x)) + \nabla_{x} log(p_{\theta}(x)) - \nabla_{x} log(p_{\theta}(y)) \newline$
$\Rightarrow \nabla_{x} log p_{\theta}(x|y) = \nabla_{x}log (p_{\theta}(y|x)) + \nabla_{x} log(p_{\theta}(x))   \space \space\space \space\space \space (*) \newline$ 
Since $p_{\theta}(y)$ is not a function in x, its gradient w.r.t. $x$ is 0.

Notice that via the rearrangement above, we are expressing the gradient of the conditional probability $log(p_{\theta}(x|y))$ in terms of the gradients of the _unconditional probability_ $log(p_{\theta}(x))$ and the _classifier_ $log(p_{\theta}(y|x))$.
It is a classifier because it is just the thing that a vanilla classification model learns: the probability of the label given the data. In this way, the diffusion model's score function can be decomposed into an unconditional probability and a classifier.


Classifier guidance tweaks the score function, to introduce a coefficient $s$ controlling the classifier term $\nabla_{x}log (p_{\theta}(y|x))$. This modification can be equivalently interpreted as perturbing the mean of the Gaussian distribution $p_{\theta}$ models; for details see Nichol et al (2022).

So, what is the effect on our model outputs of varying $s$ in the new score function? Well, we know that $s\nabla_{x}log (p_{\theta}(y|x)) = \nabla_{x} log \frac{1}{Z}p_{\theta}(y|x)^{s}$ for a constant $Z$. This is to say, multiplying the log probability of $y$ given $x$ by $s$ is proportional to raising the renormalised log probability to the power $s$. And this exponentiation has the effect of disproportionately increasing its larger values, amplifying the modes of the distribution. So, for $s>1$, the $x$ that maximises $s\nabla_{x}log (p_{\theta}(y|x)) + \nabla_{x} log(p_{\theta}(x))$ will be closer to the mode of the distribution $p_{\theta}(y|x)$, so closer to the class label $y$.

This is why classifier guidance is known to have the effect of boosting the fidelity of diffusion model outputs at the expense of diversity. And it is called classifier guidance because you are guiding the score function closer to the modes of a classifier based on the prompt.

Note that in order to apply the modified score function in practice, you need to obtain a classifier $p_{\theta}(y|x)$, which is a model trained on noisy inputs to predict the Imagenet classes. [TODO -- is classifier really referred to as p-sub-theta or does it deserve own distro annotation?]

Classifier free guidance (Ho & Salimans (2021)) is a development of classifier guidance. It pushes the model in the same direction as classifier guidance, but avoids the need to train a specialised classifier.

To use  classifier-free guidance, during training we replace the label $y$ in a condional diffusion model with a null label, $\emptyset$, a fixed proportion of the time, typically 10-20%. 

Recall that the de-noising process is modeled by $\epsilon_{\theta}(x_{t}|y)$. We replace this with $\hat{\epsilon}_{\theta}(x_{t}|y)$, a weighted combination of the original conditional denoising model and an unconditional denoising model as follows:

$$\hat{\epsilon}_{\theta}(x_{t}|y) = \epsilon_{\theta}(x_{t}|\emptyset) + s(\epsilon_{\theta}(x_{t}|y) - \epsilon_{\theta}(x_{t}|\emptyset)$$

For $s=0$, $\hat{\epsilon}_{\theta}$ is just the unconditional denoising model, and for $s=1$, it is the original conditional denoising model. But as $s>1$ increases, the model is pushed towards the conditional model.


I used stable-diffusion v1.4, which was trained with [10% text-conditioning dropout](https://huggingface.co/CompVis/stable-diffusion-v1-4), allowing me to use classifier-free guidance at inference time to push the denoising process even more strongly towards the conditioning $y$, which for me, is my new `brian_wildsmith` pseudo-word.



## Textual inversion for bias reduction
Finally, I wanted to flag a use case of textual inversion that I haven't seen discussed beyond the original paper. It is well known that generative models reproduce the biases of their training data. For instance, the prompts 'doctor' and 'scientist' disproportionately produce images that resemble men. This reflects biases in the kind of images that are uploaded to image hosting sites where the training data is collected from.

When DALL-E2 was first released, this bias recieved a lot of attention and OpenAI [scrambled to release a patch](https://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2/) that would force the model to produce more diverse images.
The first pass solution was extremely hacky. It consisted of silently adding a keyword from a list of minority groups (such as 'female', 'black') to the  prompt, as [users quickly discovered](https://twitter.com/rzhang88/status/1549472829304741888) by prompting the model with "A doctor holding a sign that says", and observing the keyword printed on the sign in the generated image. This phenomenon isn't reproducible in current versions, so clearly OpenAI have a more robust solution now. A far better solution would be to fix the training set. But retraining is computationally expensive.

As the authors of the textual inversion paper suggest, we can use the technique to update a trained model's understanding of words already in its vocabulary. Rather than using a novel token, as we do when adding a style or object to the model's vocabulary, we overwrite the embedding of an existing token, training on a small, curated set of images. 

The compute budget to do this is negligible relative to training the base model. My textual inversion above ran in about 4 hours on a GeForce GTX 1080, whereas the initial training of stable diffusion was 150,000 GPU hours on the considerably more FLOP-heavy A100. So this seems like an elegant and practical tool in the toolbelt to use in countering bias.

**References**

Ho, J. and Salimans, T. *Classifier-free diffusion guidance*, 
In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 
URL https://
openreview.net/forum?id=qw8AKxfYbI.


Gal, R, Alaluf Y., Atzmon Y., Patashnik O., Bermano A, Chechik G., and Cohen-Or D.,
*An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion*,  preprint arXiv:2208.01618 2022

Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,  Sutskever, I., Chen, M.
*GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models*, preprint https://arxiv.org/pdf/2112.10741.pdf 2022


</section>

		</div>

	</div>


	<footer class="footer">

	<div class="wrap">

		
		<a href="/" class="footer__title">
			Kate Hodesdon
		</a>
		

		<p class="footer__text">The personal website of Kate Hodesdon</p>

		<div class="footer__copyright">
			<span>© 2023 Kate Hodesdon</span>
		</div>

		<ul class="socials">
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
</ul>

	</div>

</footer>


	<!-- Javascript Assets -->
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script src="/js/plugins-min.js"></script>
	<script src="/js/duet-min.js"></script>

	
	<!-- Extra Footer JS Code -->
	
	


</body>

</html>